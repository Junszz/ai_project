{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = '../datasets/train'\n",
    "VALID_DIR = '../datasets/val'\n",
    "TEST_DIR = '../datasets/test'\n",
    "save_path = '../output'\n",
    "\n",
    "class_to_int = {\"dog\" : 0, \"cat\" : 1}\n",
    "int_to_class = {0 : \"dog\", 1 : \"cat\"}\n",
    "\n",
    "train_imgs = os.listdir(TRAIN_DIR)\n",
    "valid_imgs = os.listdir(VALID_DIR)\n",
    "test_imgs = os.listdir(TEST_DIR)\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # Require size by Resnet-18\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                        (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                        (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDogDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, imgs, class_to_int, mode = \"test\", transforms = None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.imgs = imgs\n",
    "        self.class_to_int = class_to_int\n",
    "        self.mode = mode\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image_name = self.imgs[idx]\n",
    "        \n",
    "        ### Reading, converting and normalizing image\n",
    "        #img = cv2.imread(DIR_TRAIN + image_name, cv2.IMREAD_COLOR)\n",
    "        #img = cv2.resize(img, (224,224))\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        #img /= 255.\n",
    "        img = Image.open(TEST_DIR + image_name)\n",
    "        img = img.resize((224, 224))\n",
    "                    \n",
    "        ### Apply Transforms on image\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        return img\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_train_transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\workspace\\FYP\\ai_project\\src\\new_train.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load the dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transform)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# valid_dataset = datasets.ImageFolder(root=VALID_DIR, transform=valid_transform)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m CatDogDataset(train_imgs, class_to_int, mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, transforms \u001b[39m=\u001b[39m get_train_transform())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m valid_dataset \u001b[39m=\u001b[39m CatDogDataset(val_imgs, class_to_int, mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m, transforms \u001b[39m=\u001b[39m get_val_transform())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test_dataset \u001b[39m=\u001b[39m CatDogDataset(test_imgs, class_to_int, mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, transforms \u001b[39m=\u001b[39m valid_transform)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_train_transform' is not defined"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "# train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transform)\n",
    "# valid_dataset = datasets.ImageFolder(root=VALID_DIR, transform=valid_transform)\n",
    "train_dataset = CatDogDataset(train_imgs, class_to_int, mode = \"train\", transforms = train_transform)\n",
    "valid_dataset = CatDogDataset(val_imgs, class_to_int, mode = \"val\", transforms = valid_transform)\n",
    "test_dataset = CatDogDataset(test_imgs, class_to_int, mode = \"test\", transforms = valid_transform)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    num_workers = 4,\n",
    "    batch_size = 16,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    dataset = valid_dataset,\n",
    "    num_workers = 4,\n",
    "    batch_size = 16,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    num_workers = 4,\n",
    "    batch_size = 16,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "print(f\"[INFO]: Number of training images: {len(train_dataset)}\")\n",
    "print(f\"[INFO]: Number of validation images: {len(valid_dataset)}\")\n",
    "print(f\"[INFO]: Number of test images: {len(test_dataset)}\")\n",
    "print(f\"[INFO]: Class names: {train_dataset.classes}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\65913\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\65913\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "\n",
    "model = models.resnet18(pretrained = True)\n",
    "\n",
    "# Modifying Head - classifier\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1, bias = True),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "if use_gpu:\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(torch.device('cuda'))\n",
    "else:\n",
    "    model.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.5)\n",
    "\n",
    "#Loss Function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Logs - Helpful for plotting after training finishes\n",
    "train_logs = {\"loss\" : [], \"accuracy\" : [], \"time\" : []}\n",
    "val_logs = {\"loss\" : [], \"accuracy\" : [], \"time\" : []}\n",
    "\n",
    "# Loading model to device\n",
    "model.to(device)\n",
    "\n",
    "# No of epochs \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, trues):\n",
    "    \n",
    "    ### Converting preds to 0 or 1\n",
    "    preds = [1 if preds[i] >= 0.5 else 0 for i in range(len(preds))]\n",
    "    \n",
    "    ### Calculating accuracy by comparing predictions with true labels\n",
    "    acc = [1 if preds[i] == trues[i] else 0 for i in range(len(preds))]\n",
    "    \n",
    "    ### Summing over all correct predictions\n",
    "    acc = np.sum(acc) / len(preds)\n",
    "    \n",
    "    return (acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_data_loader):\n",
    "    \n",
    "    ### Local Parameters\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ###Iterating over data loader\n",
    "    for images, labels in train_data_loader:\n",
    "        \n",
    "        #Loading images and labels to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.reshape((labels.shape[0], 1)) # [N, 1] - to match with preds shape\n",
    "        \n",
    "        #Reseting Gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward\n",
    "        preds = model(images)\n",
    "        \n",
    "        #Calculating Loss\n",
    "        _loss = criterion(preds, labels)\n",
    "        loss = _loss.item()\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "        #Calculating Accuracy\n",
    "        acc = accuracy(preds, labels)\n",
    "        epoch_acc.append(acc)\n",
    "        \n",
    "        #Backward\n",
    "        _loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ###Overall Epoch Results\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    ###Acc and Loss\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    epoch_acc = np.mean(epoch_acc)\n",
    "    \n",
    "    ###Storing results to logs\n",
    "    train_logs[\"loss\"].append(epoch_loss)\n",
    "    train_logs[\"accuracy\"].append(epoch_acc)\n",
    "    train_logs[\"time\"].append(total_time)\n",
    "        \n",
    "    return epoch_loss, epoch_acc, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_one_epoch(val_data_loader, best_val_acc):\n",
    "    \n",
    "    ### Local Parameters\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ###Iterating over data loader\n",
    "    for images, labels in val_data_loader:\n",
    "        \n",
    "        #Loading images and labels to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.reshape((labels.shape[0], 1)) # [N, 1] - to match with preds shape\n",
    "        \n",
    "        #Forward\n",
    "        preds = model(images)\n",
    "        \n",
    "        #Calculating Loss\n",
    "        _loss = criterion(preds, labels)\n",
    "        loss = _loss.item()\n",
    "        epoch_loss.append(loss)\n",
    "        \n",
    "        #Calculating Accuracy\n",
    "        acc = accuracy(preds, labels)\n",
    "        epoch_acc.append(acc)\n",
    "    \n",
    "    ###Overall Epoch Results\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    ###Acc and Loss\n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    epoch_acc = np.mean(epoch_acc)\n",
    "    \n",
    "    ###Storing results to logs\n",
    "    val_logs[\"loss\"].append(epoch_loss)\n",
    "    val_logs[\"accuracy\"].append(epoch_acc)\n",
    "    val_logs[\"time\"].append(total_time)\n",
    "    \n",
    "    ###Saving best model\n",
    "    if epoch_acc > best_val_acc:\n",
    "        best_val_acc = epoch_acc\n",
    "        torch.save(model.state_dict(),\"resnet50_best.pth\")\n",
    "        \n",
    "    return epoch_loss, epoch_acc, total_time, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x512 and 2048x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\workspace\\FYP\\ai_project\\src\\new_train.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m best_val_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m###Training\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     loss, acc, _time \u001b[39m=\u001b[39m train_one_epoch(train_data_loader)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m#Print Epoch Details\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\workspace\\FYP\\ai_project\\src\\new_train.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(train_data_loader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#Forward\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m preds \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#Calculating Loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/workspace/FYP/ai_project/src/new_train.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m _loss \u001b[39m=\u001b[39m criterion(preds, labels)\n",
      "File \u001b[1;32mc:\\Users\\65913\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\65913\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\65913\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:280\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    278\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m    279\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[0;32m    282\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\65913\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\65913\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\65913\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\65913\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x512 and 2048x1)"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    ###Training\n",
    "    loss, acc, _time = train_one_epoch(train_data_loader)\n",
    "    \n",
    "    #Print Epoch Details\n",
    "    print(\"\\nTraining\")\n",
    "    print(\"Epoch {}\".format(epoch+1))\n",
    "    print(\"Loss : {}\".format(round(loss, 4)))\n",
    "    print(\"Acc : {}\".format(round(acc, 4)))\n",
    "    print(\"Time : {}\".format(round(_time, 4)))\n",
    "    \n",
    "    ###Validation\n",
    "    loss, acc, _time, best_val_acc = val_one_epoch(val_data_loader, best_val_acc)\n",
    "    \n",
    "    #Print Epoch Details\n",
    "    print(\"\\nValidating\")\n",
    "    print(\"Epoch {}\".format(epoch+1))\n",
    "    print(\"Loss : {}\".format(round(loss, 4)))\n",
    "    print(\"Acc : {}\".format(round(acc, 4)))\n",
    "    print(\"Time : {}\".format(round(_time, 4)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting Results\n",
    "\n",
    "#Loss\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(np.arange(1, 11, 1), train_logs[\"loss\"], color = 'blue')\n",
    "plt.plot(np.arange(1, 11, 1), val_logs[\"loss\"], color = 'yellow')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "#Accuracy\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(np.arange(1, 11, 1), train_logs[\"accuracy\"], color = 'blue')\n",
    "plt.plot(np.arange(1, 11, 1), val_logs[\"accuracy\"], color = 'yellow')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10b752d122ffbd1b758811b5ed6929ef257394e7b9ef6f69ed2e28f3a3aa37ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
